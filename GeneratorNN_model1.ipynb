{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 32229,
     "status": "ok",
     "timestamp": 1602657243017,
     "user": {
      "displayName": "Denis Logwinenko",
      "photoUrl": "",
      "userId": "10619589069945453273"
     },
     "user_tz": -120
    },
    "id": "KBhcMNhYvN0l",
    "outputId": "d8765042-09a2-49b7-9758-87c46bd625e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtokentome in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from prettytable) (41.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prettytable) (0.1.7)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.50.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtokentome\n",
    "!pip install prettytable\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "kPtySk0WtHWO",
    "outputId": "fba5735e-5c36-49bb-affa-54921f152bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n",
      "   Review                                           Sentence  Professional  \\\n",
      "0       0  i have the doo wop 00 and 00 dvds, and was anx...             1   \n",
      "1       0  from the first video, which featured the crme ...             1   \n",
      "2       0  better lighting, better stage layout, and bett...             1   \n",
      "\n",
      "   Sentiment  Length  Personal  Theme  Descriptive  \n",
      "0          5       8        11     16           17  \n",
      "1          5       8        10     14           17  \n",
      "2          5       7        10     15           18  \n",
      "Training set: 817378 sents\n",
      "Test set: 16682 sents\n",
      "Byte Pair Encoding: [ 276  198 2080  113 3625  577 8717   27  109   87]\n",
      "\n",
      "Subwords: ['<PAD>', '<UNK>', '<BOS>', '<EOS>', 'â–', 'e', 't', 'a', 'i', 'o'],\n",
      "30469 subwords in total\n",
      "\n",
      "Metadata for every subword:\n",
      " [[ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]]\n",
      "X of shape (8, 50):\n",
      " [[  276   198  2080   113  3625   577  8717    27   109    87]\n",
      " [ 1527   115   325   958   109    87   685  1503   758   319]\n",
      " [  192    84  2150 13572   724   186    86   376   480   145]\n",
      " [  964 11101    87   329  2185   491  1735     3    87   176]\n",
      " [    3   120    84   918  1303   429    87 28980    33    10]\n",
      " [  435   315   333   161   178 10197   113    87   708  1452]\n",
      " [   87  1164   680    13  1287  2975   270  2541 11487   350]\n",
      " [  142  8081   113    87  2295  9805  1641     3    87  2620]]\n",
      "\n",
      "Y of shape (8, 50):\n",
      " [[  198  2080   113  3625   577  8717    27   109    87   988]\n",
      " [  115   325   958   109    87   685  1503   758   319   601]\n",
      " [   84  2150 13572   724   186    86   376   480   145  3067]\n",
      " [11101    87   329  2185   491  1735     3    87   176   266]\n",
      " [  120    84   918  1303   429    87 28980    33    10  3475]\n",
      " [  315   333   161   178 10197   113    87   708  1452 30364]\n",
      " [ 1164   680    13  1287  2975   270  2541 11487   350  4559]\n",
      " [ 8081   113    87  2295  9805  1641     3    87  2620   652]]\n",
      "\n",
      "X_meta of shape (8, 50, 6):\n",
      " [[ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]\n",
      " [ 2  5  7 10 12 17]]\n",
      "\n",
      "First 50 elements in the 1st batch are equal to original values: True\n",
      "SubwordRNN(\n",
      "  (subword_embeds): Embedding(30469, 256)\n",
      "  (meta_embeds): Embedding(19, 20)\n",
      "  (lstm): LSTM(376, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=30469, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Cell 1: ###\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np; np.random.seed(42)\n",
    "import youtokentome as yttm\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import itertools\n",
    "import operator\n",
    "import os, sys\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "\n",
    "\n",
    "### Cell 2: ###\n",
    "amazon_movies = 'amazon_movies.txt'\n",
    "corpus_small = 'corpus_small.csv'\n",
    "corpus_small_annotated = 'corpus_small_annotated.csv'\n",
    "csa_json = 'corpus_small_annotated.json'\n",
    "sentences = 'sentences.txt'\n",
    "bpe_model = 'bpe.model'\n",
    "bpe_model_small = 'bpe10K.model'\n",
    "bpe_model_medium = 'bpe20K.model'\n",
    "\n",
    "\n",
    "### Cell 3: ###\n",
    "def save_to_path(path, extension_of_old='.txt', extension_of_new='.csv') -> str:\n",
    "    \"\"\"\n",
    "    Function checks if some file in path already exists and if so, it adds an\n",
    "    index before the extension.\n",
    "    \n",
    "    Function has 2 further parameters:\n",
    "        extension_of_old --> defines the extension of the original file\n",
    "        extension_of_new --> defines the extension of the new file to be saved\n",
    "        \n",
    "    returns the new save path\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match(f'(.+){extension_of_old}?', path)\n",
    "    save_path = m.group(1) + f'{extension_of_new}'    \n",
    "    j = 1\n",
    "    while os.path.isfile(save_path):       \n",
    "        save_path = m.group(1) + f'{j}{extension_of_new}'\n",
    "        j += 1\n",
    "        \n",
    "    return save_path\n",
    "\n",
    "\n",
    "### Cell 4: ###\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU!')\n",
    "\n",
    "\n",
    "### Cell 5: ###\n",
    "sentences_df = pd.read_csv(corpus_small_annotated, sep='\\t', header=0, encoding='utf-8', warn_bad_lines=True)\n",
    "sentences_df['Sentence'] = sentences_df['Sentence'].astype(str).str.lower()\n",
    "sentences_meta = sentences_df.iloc[:,2:]\n",
    "\n",
    "sents_list = sentences_df['Sentence'].tolist() # Converting the column with sentences into a list of strings\n",
    "\n",
    "print(sentences_df.head(3))\n",
    "\n",
    "corpus_len = len(sents_list)\n",
    "array = np.arange(start=0, stop=corpus_len) # sentence indices range   \n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "train_size = int(0.98*corpus_len)\n",
    "test_size = corpus_len - train_size\n",
    "                 \n",
    "train_set_idx = rng.choice(array, size=train_size, replace=False)\n",
    "\n",
    "test_set_idx = np.setdiff1d(array, train_set_idx) # Unique values in array1 that are not in train_set_idx\n",
    "\n",
    "print(f'Training set: {train_size} sents\\nTest set: {test_size} sents')\n",
    "\n",
    "# Some sanity checks:\n",
    "assert len(train_set_idx) == train_size\n",
    "assert len(test_set_idx) == test_size\n",
    "\n",
    "\n",
    "### Cell 6: ###\n",
    "# Dividing the list of sentences into train/test/dev sets:\n",
    "sents_list_train = [sents_list[idx] for idx in train_set_idx]\n",
    "sents_list_test = [sents_list[idx] for idx in test_set_idx]\n",
    "\n",
    "# Converting the metadata-part of sentences_df dataframe into a list of lists\n",
    "metadata = sentences_meta.values.tolist()\n",
    "\n",
    "# Dividing the list with the metadata into train/test/dev sets:\n",
    "metadata_train = [metadata[idx] for idx in train_set_idx]\n",
    "metadata_test = [metadata[idx] for idx in test_set_idx]\n",
    "\n",
    "# Loading our Byte Pair Encoding model and initialising its vocabulary:\n",
    "bpe = yttm.BPE(model=bpe_model)\n",
    "subwords = bpe.vocab()\n",
    "\n",
    "# Encoding the sentences in the training set: \n",
    "encoded_train = bpe.encode(sents_list_train, output_type=yttm.OutputType.ID, eos=True)\n",
    "# Repeating the sentence metadata for every single subword in the sentence: \n",
    "metadata_train = np.array([metadata_train[i] for i in range(len(encoded_train)) for _ in range(len(encoded_train[i]))])\n",
    "encoded_train = np.array([id_code for sent in encoded_train for id_code in sent]) # flattening the array of id codes\n",
    "\n",
    "# Encoding the sentences in the test set: \n",
    "encoded_test = bpe.encode(sents_list_test, output_type=yttm.OutputType.ID, eos=True)\n",
    "# Repeating the sentence metadata for every single subword in the sentence: \n",
    "metadata_test = np.array([metadata_test[i] for i in range(len(encoded_test)) for _ in range(len(encoded_test[i]))])\n",
    "encoded_test = np.array([id_code for sent in encoded_test for id_code in sent]) # flattening the array of id codes\n",
    "\n",
    "print('Byte Pair Encoding:', encoded_train[:10])\n",
    "print(f'\\nSubwords: {subwords[:10]},\\n{len(subwords)} subwords in total\\n')\n",
    "\n",
    "print('Metadata for every subword:\\n', metadata_train[:10])\n",
    "\n",
    "\n",
    "### Cell 7: ###\n",
    "def get_batches(arr, arr_meta, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Create a generator that returns batches of size\n",
    "    batch_size x seq_length from arr.\n",
    "       \n",
    "    Arguments\n",
    "    ---------\n",
    "    arr: Array you want to make batches from\n",
    "    arr_meta: Array with metadata annotations of the same lenght as arr \n",
    "    batch_size: Batch size, the number of sequences per batch\n",
    "    seq_length: Number of encoded subwords in a sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(arr) == len(arr_meta)\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make, // integer division, round down:\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough subwords to make full batches:\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows, n. of first row is the batch size, the other length is inferred:\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # Doing the same with the metadata:\n",
    "    arr_meta = arr_meta[:n_batches * batch_size_total].reshape((batch_size, -1, 6)) # Each subword has 6 metadata features\n",
    "    \n",
    "    # iterate through the array, one sequence at a time:\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features:\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # Metadata:\n",
    "        x_meta = arr_meta[:, n:n+seq_length]\n",
    "        # The targets, shifted to the right by one:\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, x_meta, y \n",
    "        \n",
    "batches = get_batches(encoded_train, metadata_train, 8, 50)\n",
    "x, x_meta, y = next(batches)\n",
    "\n",
    "# Printing out the first 10 items in a sequence:\n",
    "print(f'X of shape {x.shape}:\\n', x[:8, :10])\n",
    "print(f'\\nY of shape {y.shape}:\\n', y[:8, :10])\n",
    "\n",
    "# Printing out the first 10 metadata arrays:\n",
    "print(f'\\nX_meta of shape {x_meta.shape}:\\n', x_meta[0, :10])\n",
    "\n",
    "# Checking if the reshapes make sense, idx of 2nd batch = n_batches * batch_size_total / batch_size:\n",
    "print('\\nFirst 50 elements in the 1st batch are equal to original values:', np.array_equal(x_meta[0, :50], metadata_train[:50]))\n",
    "\n",
    "\n",
    "### Cell 8: ###\n",
    "def predict(net, subword, metadata, h=None, temp=0.9, top_k=None):\n",
    "    \"\"\" \n",
    "    Given a subword and its metadata predict the next subword.\n",
    "    Returns the predicted subword and the hidden state.\n",
    "    \"\"\"\n",
    "        \n",
    "    # tensor inputs:\n",
    "    x = np.array([[subword]])\n",
    "    inputs, meta = torch.from_numpy(x), \\\n",
    "                   torch.from_numpy(metadata)\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs, meta = inputs.to(device='cuda', dtype=torch.long), \\\n",
    "                       meta.to(device='cuda', dtype=torch.long)\n",
    "\n",
    "    # detach hidden state from history:\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    # get the output of the model:\n",
    "    out, h = net(inputs, meta, h)\n",
    "\n",
    "    # get the subwords probabilities\n",
    "    # apply softmax to get p probabilities for the likely next subword giving x:\n",
    "    p = F.softmax(out/temp, dim=1).data\n",
    "    if train_on_gpu:\n",
    "        p = p.cpu() # move to cpu\n",
    "\n",
    "    # get top subwords\n",
    "    # considering the k most probable subwords with topk method:\n",
    "    if top_k is None:\n",
    "        top_sw = np.arange(len(net.subwords))\n",
    "    else:\n",
    "        p, top_sw = p.topk(top_k)\n",
    "        top_sw = top_sw.numpy().squeeze()\n",
    "\n",
    "    # select the likely next subword with some element of randomness:\n",
    "    p = p.numpy().squeeze()\n",
    "    subword = np.random.choice(top_sw, p=p/p.sum())\n",
    "\n",
    "    # return the encoded value of the predicted subword and the hidden state:\n",
    "    return subword, h\n",
    "  \n",
    "\n",
    "def sample(net, size=300, num_sents=5, prime='this movie', temp=0.9,\n",
    "           metadata=[1, 5, 8, 11, 13, 17], top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    # First off, run through the prime subwords:\n",
    "    subwords = net.bpe.encode(prime, output_type=yttm.OutputType.ID)\n",
    "    metadata = np.array([[metadata]]) # Reshaping to (1, 1, 6) so that the dims correspond to those from get_batches()\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    for sw in subwords:\n",
    "        subword, h = predict(net, sw, metadata, h, temp=temp, top_k=top_k)\n",
    "\n",
    "    subwords.append(subword) # appending the subword that comes after the last prime subword\n",
    "    \n",
    "    # Now pass in the previous subword and get a new one\n",
    "    i = 0\n",
    "    while (subwords.count(3) < num_sents) and (i < size): # 3 is the index of '<EOS>'\n",
    "        subword, h = predict(net, subwords[-1], metadata, h, temp=temp, top_k=top_k)\n",
    "        subwords.append(subword)\n",
    "        i += 1 \n",
    "        \n",
    "    subwords = list(filter(lambda a: a != 3, subwords)) # remove all occurrences of '<EOS>' from the final output\n",
    "    return net.bpe.decode(subwords)[0]\n",
    "\n",
    "\n",
    "### Cell 9: ###\n",
    "class SubwordRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, subwords=None, bpe=None, embedding_dims=None, \\\n",
    "                 n_hidden=1024, n_layers=2, drop_prob=0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # saving the bpe-model to be able to use its encoding/decoding functions when generating some text:\n",
    "        self.bpe = bpe\n",
    "        self.subwords = subwords\n",
    "        \n",
    "        # defining 2 Embedding layers: one for subwords and another for metadata:\n",
    "        if embedding_dims is None:\n",
    "            embedding_dims = [256, 20]\n",
    "            \n",
    "        subword_emb_dim, metadata_emb_dim = embedding_dims\n",
    "        \n",
    "        self.subword_embeds = nn.Embedding(len(self.subwords), subword_emb_dim)\n",
    "        self.meta_embeds = nn.Embedding(19, metadata_emb_dim) # 18 - total number of metadata features\n",
    "        \n",
    "        embedding_dim = subword_emb_dim + 6*metadata_emb_dim\n",
    "        # defining the LSTM:\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # define a dropout layer:\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # define the final, fully-connected output layer:\n",
    "        self.fc = nn.Linear(n_hidden, len(self.subwords))\n",
    "        \n",
    "    \n",
    "    def forward(self, x, x_meta, hidden):\n",
    "        \"\"\" \n",
    "        Forward pass through the network. \n",
    "        These inputs are x, and the hidden/cell state `hidden`. \n",
    "        \"\"\"\n",
    "        # Creating an embedding by concatenating a subword embedding and a metadata embedding:\n",
    "        subword_embeds = self.subword_embeds(x)\n",
    "        meta_embeds = self.meta_embeds(x_meta).view(*x.size(), -1)\n",
    "    \n",
    "        embeds = torch.cat((subword_embeds, meta_embeds), dim=2)\n",
    "        \n",
    "        # Get the outputs and the new hidden state from the lstm:\n",
    "        r_output, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # pass through a dropout layer:\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view:\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        # put x through the fully-connected layer:\n",
    "        out = self.fc(out)       \n",
    "        \n",
    "        # return the final output and the hidden state:\n",
    "        return out, hidden    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state \n",
    "        \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM:\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device='cuda'),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device='cuda'))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "### Cell 10: ###\n",
    "def train(net, data, metadata, epochs=10, batch_size=10, seq_length=50, lr=0.001, \n",
    "          clip=5, val_frac=0.1, print_every=1, current_epoch=1, save_every=10):\n",
    "    \"\"\" \n",
    "    Training a network \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        \n",
    "    net: SubwordRNN network\n",
    "    data: text data to train the network\n",
    "    epochs: Number of epochs to train\n",
    "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "    seq_length: Number of character steps per mini-batch\n",
    "    lr: learning rate\n",
    "    clip: gradient clipping\n",
    "    val_frac: Fraction of data to hold out for validation\n",
    "    print_every: Number of steps for printing training and validation loss    \n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create training and validation data:\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    metadata, val_metadata = metadata[:val_idx], metadata[val_idx:]\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.to(device='cuda')\n",
    "    \n",
    "    counter = 0\n",
    "    n_subwords = len(net.subwords)\n",
    "    for e in range(current_epoch, epochs+current_epoch):\n",
    "        # Initialize hidden state:\n",
    "        h = net.init_hidden(batch_size)\n",
    "        losses = []\n",
    "        \n",
    "        for x, x_meta, y in get_batches(data, metadata, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # Convert our data to Torch tensors:\n",
    "            inputs, meta, targets = torch.from_numpy(x), \\\n",
    "                                    torch.from_numpy(x_meta), \\\n",
    "                                    torch.from_numpy(y)\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs, meta, targets = inputs.to(device='cuda', dtype=torch.long), \\\n",
    "                                        meta.to(device='cuda', dtype=torch.long), \\\n",
    "                                        targets.to(device='cuda', dtype=torch.long)\n",
    "        \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history:\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # Zero accumulated gradients:\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # Get the output from the model:\n",
    "            output, h = net(inputs, meta, h)\n",
    "            \n",
    "            # Calculate the loss and perform backprop:           \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs:\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "        # Loss stats:\n",
    "        if e % print_every == 0:\n",
    "            # Get validation loss:\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for x, x_meta, y in get_batches(val_data, val_metadata, batch_size, seq_length):\n",
    "                # Convert our data to Torch tensors:\n",
    "                x, x_meta, y = torch.from_numpy(x), \\\n",
    "                               torch.from_numpy(x_meta), \\\n",
    "                               torch.from_numpy(y)\n",
    "                \n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                inputs, meta, targets = x, x_meta, y\n",
    "                if train_on_gpu:\n",
    "                    inputs, meta, targets = inputs.to(device='cuda', dtype=torch.long), \\\n",
    "                                            meta.to(device='cuda', dtype=torch.long), \\\n",
    "                                            targets.to(device='cuda', dtype=torch.long)\n",
    "\n",
    "                output, val_h = net(inputs, meta, val_h)\n",
    "                val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            net.train() # reset to train mode after iterating through validation data\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Printing out some training statistics:\n",
    "            print(f'Epoch: {e:2}/{epochs}...',\n",
    "                  f'Step: {counter:6}...',\n",
    "                  f'Mean_Loss: {np.mean(losses):.3f}...',\n",
    "                  f'Val_Loss: {mean_val_loss:.3f}')\n",
    "            \n",
    "            # Generating text:\n",
    "            generated = sample(net, size=300, prime='this movie', temp=0.9,\n",
    "                               metadata=[2, 5, 7, 10, 12, 18], top_k=None) # generating text            \n",
    "            generated = f'Generated at epoch {e}:\\n{generated}\\n\\n' # generated text with a header\n",
    "            \n",
    "            save_dir = 'Generated Text' # directory where the generated text at epoch e will be saved\n",
    "            if not os.path.isdir(save_dir): # if there is no such directory, one will be created\n",
    "                os.mkdir(save_dir) \n",
    "            with open(f'{save_dir}/generated_text.txt', 'a', encoding='utf-8', errors='replace') as f:\n",
    "                f.write(generated)\n",
    "                \n",
    "        # Saving our model:        \n",
    "        if e % save_every == 0:\n",
    "            net.eval() # VERY IMPORTANT\n",
    "            \n",
    "            if e >= 1 and e < 10:\n",
    "                model = f'lstm_0{e}_epoch.pt'\n",
    "            else:\n",
    "                model = f'lstm_{e}_epoch.pt'\n",
    "            \n",
    "            checkpoint = {'epoch': e,\n",
    "                          'n_hidden': net.n_hidden,\n",
    "                          'n_layers': net.n_layers,\n",
    "                          'model_state_dict': net.state_dict(),\n",
    "                          'opt_state_dict': opt.state_dict(),\n",
    "                          'loss': loss}\n",
    "\n",
    "            with open(model, 'wb') as f:\n",
    "                torch.save(checkpoint, f)\n",
    "                \n",
    "            net.train() # VERY IMPORTANT   \n",
    "\n",
    "\n",
    "### Cell 11: ###\n",
    "# Defining and printing out the NN properties:\n",
    "embedding_dims = [256, 20] #[128, 10]\n",
    "drop_prob = 0.5 #0.5\n",
    "n_hidden= 1024 #1024\n",
    "n_layers=3 #3\n",
    "lr=0.0001 #0.001 seems to overfit\n",
    "\n",
    "net = SubwordRNN(subwords=subwords, bpe=bpe, embedding_dims=embedding_dims, \n",
    "                 n_hidden=n_hidden, n_layers=n_layers, drop_prob=drop_prob)\n",
    "print(net)\n",
    "\n",
    "batch_size = 32 #64 \n",
    "seq_length = 160 #160 \n",
    "n_epochs = 10 #10\n",
    "\n",
    "# Training the model: \n",
    "# use encoded_train[:100000], metadata_train[:100000] for debugging!\n",
    "#train(net, encoded_train, metadata_train, epochs=n_epochs, batch_size=batch_size, \n",
    "      #seq_length=seq_length, lr=lr, print_every=1, save_every=1, current_epoch=1) # trained in Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubwordRNN(\n",
      "  (subword_embeds): Embedding(30469, 256)\n",
      "  (meta_embeds): Embedding(19, 20)\n",
      "  (lstm): LSTM(376, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=30469, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "path_to_model = 'lstm_10_epoch.pt'\n",
    "\n",
    "checkpoint = torch.load(path_to_model)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(net)\n",
    "#train(net, encoded_train, metadata_train, epochs=n_epochs, batch_size=batch_size, \n",
    "      #seq_length=seq_length, lr=lr, print_every=1, save_every=1, current_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DwmmYxPJuxQQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------+\n",
      "|        Modules        | Parameters |\n",
      "+-----------------------+------------+\n",
      "| subword_embeds.weight |  7800064   |\n",
      "|   meta_embeds.weight  |    380     |\n",
      "|   lstm.weight_ih_l0   |  1540096   |\n",
      "|   lstm.weight_hh_l0   |  4194304   |\n",
      "|    lstm.bias_ih_l0    |    4096    |\n",
      "|    lstm.bias_hh_l0    |    4096    |\n",
      "|   lstm.weight_ih_l1   |  4194304   |\n",
      "|   lstm.weight_hh_l1   |  4194304   |\n",
      "|    lstm.bias_ih_l1    |    4096    |\n",
      "|    lstm.bias_hh_l1    |    4096    |\n",
      "|   lstm.weight_ih_l2   |  4194304   |\n",
      "|   lstm.weight_hh_l2   |  4194304   |\n",
      "|    lstm.bias_ih_l2    |    4096    |\n",
      "|    lstm.bias_hh_l2    |    4096    |\n",
      "|       fc.weight       |  31200256  |\n",
      "|        fc.bias        |   30469    |\n",
      "+-----------------------+------------+\n",
      "Total Trainable Params: 61567361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61567361"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_experiments = [[2, 5, 8, 11, 12, 17], \n",
    "                        [1, 5, 8, 11, 12, 17], \n",
    "                        [2, 3, 8, 11, 12, 17], \n",
    "                        [2, 5, 6, 11, 12, 17], \n",
    "                        [2, 5, 8, 10, 12, 17], \n",
    "                        [2, 5, 8, 11, 15, 17], \n",
    "                        [2, 5, 8, 11, 12, 18]]\n",
    "\n",
    "for cur_meta in metadata_experiments:\n",
    "    generated = sample(net, size=300, num_sents=2, prime='this movie', temp=0.6,\n",
    "                       metadata=cur_meta, top_k=None)\n",
    "    generated = f'Metadata: {\" \".join([str(el) for el in cur_meta])}\\n{generated}\\n\\n'\n",
    "    with open(f'experiments.txt', 'a', encoding='utf-8', errors='replace') as f:\n",
    "        f.write(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the trained model: 161.68\n"
     ]
    }
   ],
   "source": [
    "def perplexity(net, encoded_test, metadata_test, batch_size=16, seq_length=160):\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "        \n",
    "    net.eval()    \n",
    "    h = net.init_hidden(batch_size) \n",
    "    logs_sum, N = 0, 0\n",
    "    \n",
    "    for x, x_meta, y in get_batches(encoded_test, metadata_test, 16, 160):                \n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs = torch.from_numpy(x).to(device='cuda', dtype=torch.long)\n",
    "            meta = torch.from_numpy(x_meta).to(device='cuda', dtype=torch.long)\n",
    "            targets = torch.from_numpy(y).to(device='cuda', dtype=torch.long)\n",
    "\n",
    "        N += batch_size*seq_length\n",
    "        output, h = net(inputs, meta, h)\n",
    "        loss = F.cross_entropy(output, targets.reshape(batch_size*seq_length), \n",
    "                               ignore_index=0, reduction='sum')\n",
    "        logs_sum += loss.item()\n",
    "        \n",
    "    prplxt = np.exp(logs_sum/N)\n",
    "    \n",
    "    return prplxt\n",
    "\n",
    "print(f'Perplexity of the trained model: {perplexity(net, encoded_test, metadata_test):.2f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GeneratorNN_ver3.ipynb",
   "provenance": [
    {
     "file_id": "1VIR6Br6-iUSeKYCVMfS_hcMyVP2MwybR",
     "timestamp": 1600709535732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
