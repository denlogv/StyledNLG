{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 32229,
     "status": "ok",
     "timestamp": 1602657243017,
     "user": {
      "displayName": "Denis Logwinenko",
      "photoUrl": "",
      "userId": "10619589069945453273"
     },
     "user_tz": -120
    },
    "id": "KBhcMNhYvN0l",
    "outputId": "d8765042-09a2-49b7-9758-87c46bd625e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtokentome in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prettytable) (0.1.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from prettytable) (41.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.50.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtokentome\n",
    "!pip install prettytable\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "kPtySk0WtHWO",
    "outputId": "fba5735e-5c36-49bb-affa-54921f152bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n",
      "Training set: 97998 reviews\n",
      "Test set: 2000 reviews\n",
      "Byte Pair Encoding for the 1st review:\n",
      " [217, 2855, 482, 126, 170, 4118, 29044, 86, 1253, 767, 115, 340, 898, 186, 482, 126, 170, 1304, 142, 84, 1018, 1627, 86, 764, 1738, 1527, 505, 3, 87, 2237, 800, 950, 593, 305, 5367, 126, 3810, 186, 86, 6560, 120, 276, 867, 86, 530, 5713, 3974, 109, 4156, 2666, 329, 4065, 4266, 3, 86, 170, 192, 17095, 448, 3, 310, 7066, 305, 170, 87, 4120, 120, 87, 510, 1021, 689, 493, 1650, 6225, 1943, 35, 3777, 87, 14839, 2159, 11504, 24, 245, 92, 456, 354, 109, 980, 26, 3, 21504, 6457, 19121, 87, 9748, 27, 2061, 2078, 16363, 3, 86, 4122, 319, 120, 5681, 1112, 138, 444, 109, 14173, 1233, 4821, 297, 1988, 3, 315, 84, 292, 4587, 87, 3623, 113, 87, 1954, 3185, 493, 1552, 3185, 170, 26510, 7196, 3]\n",
      "\n",
      "Metadata for the first 10 subwords of the 1st review:\n",
      " [[1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17], [1, 5, 8, 11, 12, 17]]\n",
      "\n",
      "Subwords: ['<PAD>', '<UNK>', '<BOS>', '<EOS>', '▁', 'e', 't', 'a', 'i', 'o'],\n",
      "30469 subwords in total\n",
      "\n",
      "X of torch.Size([16, 51]):\n",
      " tensor([[  997,   138,   479,     3,    86,   397,   109,  4562,  1237,   505],\n",
      "        [  138,   122,    87,   406,   165,   886,   606,   560,   145,   266],\n",
      "        [   86,   170,    84,   562,  3677,     3,    86,  1035,    87,  5294],\n",
      "        [  267, 11784,  2267,  3942,  6624,   109,    86,   288,  1544,   276],\n",
      "        [   87,   440,   113,    87,  1741,    38,    19, 23417,  1516,     3],\n",
      "        [16967,   185,   163,  1023, 16967, 22859,   120,    87,   778, 29510],\n",
      "        [  626,   122,    87,  4787,   120,    87,  1861,   113,    87,  6986],\n",
      "        [  267,   510,   708,  2697,   376,   530,   138,   165,   109,   216]])\n",
      "\n",
      "Y of torch.Size([16, 51]):\n",
      " tensor([[  138,   479,     3,    86,   397,   109,  4562,  1237,   505,     3],\n",
      "        [  122,    87,   406,   165,   886,   606,   560,   145,   266,   835],\n",
      "        [  170,    84,   562,  3677,     3,    86,  1035,    87,  5294,  3806],\n",
      "        [11784,  2267,  3942,  6624,   109,    86,   288,  1544,   276,   419],\n",
      "        [  440,   113,    87,  1741,    38,    19, 23417,  1516,     3,  2764],\n",
      "        [  185,   163,  1023, 16967, 22859,   120,    87,   778, 29510,     3],\n",
      "        [  122,    87,  4787,   120,    87,  1861,   113,    87,  6986,  2669],\n",
      "        [  510,   708,  2697,   376,   530,   138,   165,   109,   216,   560]])\n",
      "\n",
      "X_meta of torch.Size([16, 51, 6]):\n",
      " tensor([[ 2,  3,  6, 10, 16, 17],\n",
      "        [ 2,  3,  6, 10, 16, 17],\n",
      "        [ 2,  3,  6, 10, 16, 17],\n",
      "        [ 2,  3,  6, 10, 16, 17],\n",
      "        [ 2,  3,  6, 11, 16, 17],\n",
      "        [ 2,  3,  6, 11, 16, 17],\n",
      "        [ 2,  3,  6, 11, 16, 17],\n",
      "        [ 2,  3,  6, 11, 16, 17],\n",
      "        [ 2,  3,  6, 11, 16, 17],\n",
      "        [ 2,  3,  6, 11, 16, 17]])\n",
      "\n",
      "Batch sizes of torch.Size([16]):\n",
      " tensor([51, 51, 51, 50, 46, 44, 41, 38, 38, 36, 33, 23,  4,  4,  3,  3])\n",
      "SubwordRNN(\n",
      "  (sw_emb_layer): Embedding(30469, 256)\n",
      "  (md_emb_layer): Embedding(19, 20)\n",
      "  (lstm): LSTM(376, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=30469, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Cell 1: ###\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np; np.random.seed(42)\n",
    "import youtokentome as yttm\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import itertools\n",
    "import operator\n",
    "import os, sys\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "\n",
    "\n",
    "### Cell 2: ###\n",
    "amazon_movies = 'amazon_movies.txt'\n",
    "corpus_small = 'corpus_small.csv'\n",
    "corpus_small_annotated = 'corpus_small_annotated.csv'\n",
    "csa_json = 'corpus_small_annotated.json'\n",
    "sentences = 'sentences.txt'\n",
    "bpe_model = 'bpe.model'\n",
    "bpe_model_small = 'bpe10K.model'\n",
    "bpe_model_medium = 'bpe20K.model'\n",
    "\n",
    "\n",
    "### Cell 3: ###\n",
    "def save_to_path(path, extension_of_old='.txt', extension_of_new='.csv') -> str:\n",
    "    \"\"\"\n",
    "    Function checks if some file in path already exists and if so, it adds an\n",
    "    index before the extension.\n",
    "    \n",
    "    Function has 2 further parameters:\n",
    "        extension_of_old --> defines the extension of the original file\n",
    "        extension_of_new --> defines the extension of the new file to be saved\n",
    "        \n",
    "    returns the new save path\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match(f'(.+){extension_of_old}?', path)\n",
    "    save_path = m.group(1) + f'{extension_of_new}'    \n",
    "    j = 1\n",
    "    while os.path.isfile(save_path):       \n",
    "        save_path = m.group(1) + f'{j}{extension_of_new}'\n",
    "        j += 1\n",
    "       \n",
    "    return save_path\n",
    "\n",
    "\n",
    "### Cell 4: ###\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU!')\n",
    "\n",
    "\n",
    "### Cell 5: ###\n",
    "with open(csa_json) as f:\n",
    "    dic = json.load(f)\n",
    "    sents_by_reviews = dic['sents_by_reviews']\n",
    "    meta_by_reviews = dic['meta_by_reviews']\n",
    "    del dic\n",
    "\n",
    "corpus_len = len(sents_by_reviews)\n",
    "array = np.arange(start=0, stop=corpus_len) # sentence indices range   \n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "train_size = int(0.98*corpus_len)\n",
    "test_size = corpus_len - train_size\n",
    "                 \n",
    "train_set_idx = rng.choice(array, size=train_size, replace=False)\n",
    "\n",
    "test_set_idx = np.setdiff1d(array, train_set_idx) # Unique values in array1 that are not in train_set_idx\n",
    "\n",
    "print(f'Training set: {train_size} reviews\\nTest set: {test_size} reviews')\n",
    "\n",
    "# Some sanity checks:\n",
    "assert len(train_set_idx) == train_size\n",
    "assert len(test_set_idx) == test_size\n",
    "\n",
    "\n",
    "### Cell 6: ###\n",
    "# Getting metadata for every subword in a review:\n",
    "def format_metadata(encoded, metadata):\n",
    "    metadata_bysw = []\n",
    "    for i, review in enumerate(encoded):\n",
    "        metadata_bysw.append([])\n",
    "        for j, sent in enumerate(review):\n",
    "            for sw in sent:\n",
    "                metadata_bysw[i].append(metadata[i][j])\n",
    "                \n",
    "    return metadata_bysw\n",
    "\n",
    "# Dividing the list of sentences into train and test sets:\n",
    "sents_list_train = [sents_by_reviews[idx] for idx in train_set_idx]\n",
    "sents_list_test = [sents_by_reviews[idx] for idx in test_set_idx]\n",
    "\n",
    "# Dividing the list with the metadata into train and test sets:\n",
    "metadata_train = [meta_by_reviews[idx] for idx in train_set_idx]\n",
    "metadata_test = [meta_by_reviews[idx] for idx in test_set_idx]\n",
    "\n",
    "# Loading our Byte Pair Encoding model and initialising its vocabulary:\n",
    "bpe = yttm.BPE(model=bpe_model)\n",
    "subwords = bpe.vocab()\n",
    "\n",
    "# Encoding the sentences in the training set: \n",
    "encoded_train = [bpe.encode(sents, output_type=yttm.OutputType.ID, eos=True) for sents in sents_list_train]\n",
    "metadata_train = format_metadata(encoded_train, metadata_train)\n",
    "\n",
    "# Flattening the list for each subword in every sentence of every review because after the metadata is collected \n",
    "# we don't need sentence boundaries represented in the list structure any more:  \n",
    "encoded_train = [[sw for sent in review for sw in sent] for review in encoded_train]\n",
    "\n",
    "# Doing the same on the test set: \n",
    "encoded_test = [bpe.encode(sents, output_type=yttm.OutputType.ID, eos=True) for sents in sents_list_test]\n",
    "metadata_test = format_metadata(encoded_test, metadata_test)\n",
    "encoded_test = [[sw for sent in review for sw in sent] for review in encoded_test]\n",
    "\n",
    "print('Byte Pair Encoding for the 1st review:\\n', encoded_train[0])\n",
    "print('\\nMetadata for the first 10 subwords of the 1st review:\\n', metadata_train[0][:10])\n",
    "print(f'\\nSubwords: {subwords[:10]},\\n{len(subwords)} subwords in total\\n')\n",
    "\n",
    "\n",
    "### Cell 7: ###\n",
    "def get_batches(li, li_meta, batch_size=16, seq_length=160, shuffle=False):\n",
    "    \"\"\"\n",
    "    Create a generator that returns batches of size\n",
    "    batch_size x seq_length from li\n",
    "    seq_length -> length of the longest sequence in batch\n",
    "       \n",
    "    Arguments\n",
    "    ---------\n",
    "    li:         List you want to make batches from\n",
    "    li_meta:    List with metadata annotations of the same length as li\n",
    "    batch_size: Batch size, the number of sequences per batch\n",
    "    shuffle:    Whether to shuffle li, li_meta <- is not recommended because of \n",
    "                            the unresolved memory issues while padding  \n",
    "    \n",
    "    Output\n",
    "    ---------\n",
    "    x:      Inputs\n",
    "    x_meta: Metadata for inputs\n",
    "    y:      Targets\n",
    "    \"\"\"\n",
    "    li_len, meta_len = len(li), len(li_meta)\n",
    "    assert li_len == meta_len\n",
    "    li = deepcopy(li)\n",
    "    li_meta = deepcopy(li_meta)\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(random.randint(0, li_len))\n",
    "        set_idx = np.arange(start=0, stop=li_len)\n",
    "        rng.shuffle(set_idx)\n",
    "        li = [li[idx] for idx in set_idx]\n",
    "        li_meta = [li_meta[idx] for idx in set_idx]\n",
    "    else:        \n",
    "        sort_key = lambda x: len(x)\n",
    "        li.sort(key=sort_key, reverse=True)\n",
    "        li_meta.sort(key=sort_key, reverse=True)\n",
    "        \n",
    "    total_num_revs = li_len-li_len%batch_size\n",
    "    li, li_meta = li[:total_num_revs], li_meta[:total_num_revs]\n",
    "    \n",
    "    for bs in range(0, total_num_revs, batch_size):\n",
    "        batch_revs = li[bs:bs+batch_size]\n",
    "        batch_meta = li_meta[bs:bs+batch_size]\n",
    "        \n",
    "        #seq_lengths_revs = torch.LongTensor(list(map(len, batch_revs)))\n",
    "        #seq_lengths_meta = torch.LongTensor(list(map(len, batch_meta)))\n",
    "        #print(seq_lengths_revs, seq_lengths_meta)\n",
    "        #return \n",
    "        \n",
    "        batch_revs = [torch.tensor(rev) for rev in batch_revs] # => we get a list of L tensors of torch.Size([*])\n",
    "        batch_meta = [torch.tensor(rev) for rev in batch_meta]\n",
    "        batch_revs = pad_sequence(batch_revs, batch_first=True) # -> torch.Size([batch_size, longest_review])\n",
    "        batch_meta = pad_sequence(batch_meta, batch_first=True)\n",
    "        longest = max(batch_revs.size())\n",
    "\n",
    "        try:\n",
    "            assert batch_meta.size()[:-1] == batch_revs.size()\n",
    "        except AssertionError:\n",
    "            print(f'Meta: {batch_meta.size()}, Rev: {batch_revs.size()}', file=sys.stderr)\n",
    "            sys.exit()\n",
    " \n",
    "        if longest <= seq_length:\n",
    "            a = batch_revs[:,:-1]\n",
    "            b = batch_meta[:,:-1]\n",
    "            c = batch_revs[:,1:]\n",
    "            a_lens = (a != 0).sum(dim=1)\n",
    "            yield a, b, c, a_lens\n",
    "        else:\n",
    "            last = longest-longest%seq_length # 160*n, 160*n - 1, idx_last = 160*n-1\n",
    "            for idx in range(0, last-seq_length, seq_length):\n",
    "                target_idx = idx + 1 # last idx = longest-1-seq_length\n",
    "                a = batch_revs[:,idx:idx+seq_length] #.numpy()\n",
    "                b = batch_meta[:,idx:idx+seq_length]\n",
    "                c = batch_revs[:,target_idx:target_idx+seq_length]\n",
    "                #a_lens = torch.count_nonzero(a, dim=1) pytorch 1.7\n",
    "                # it is better to clamp the output later to have real sizes to fill those unclamped outputs of the LSTM hidden layer with 0s\n",
    "                a_lens = (a != 0).sum(dim=1) #.clamp(min=1, max=seq_length) \n",
    "                yield a, b, c, a_lens\n",
    "\n",
    "                    \n",
    "batches = get_batches(encoded_train[:64], metadata_train[:64], batch_size=16, shuffle=False)\n",
    "\n",
    "for e in range(1): # Making sure that batch sizes are right \n",
    "    for x, x_meta, y, bs in batches:\n",
    "        s1, s2, s3 = x.size()[-1], x_meta.size()[1], y.size()[-1]\n",
    "        if s1 != s2 or s2 != s3 or s1 != s3:\n",
    "            print(s1, s2, s3)\n",
    "\n",
    "# Printing out the first 10 items in a sequence:\n",
    "print(f'X of {x.shape}:\\n', x[:8, :10])\n",
    "print(f'\\nY of {y.shape}:\\n', y[:8, :10])\n",
    "\n",
    "# Printing out the first 10 metadata arrays:\n",
    "print(f'\\nX_meta of {x_meta.shape}:\\n', x_meta[0, :10])\n",
    "\n",
    "# Printing out the batch sizes for the last batch:\n",
    "print(f'\\nBatch sizes of {bs.shape}:\\n', bs)\n",
    "\n",
    "\n",
    "### Cell 8: ###\n",
    "def predict(net, subword, metadata, h=None, temp=0.9, top_k=None, allowed_idx=None):\n",
    "    \"\"\" \n",
    "    Given a subword and its metadata predict the next subword.\n",
    "    Returns the predicted subword and the hidden state.\n",
    "    \"\"\"\n",
    "    # tensor inputs:\n",
    "    subword = torch.tensor([[subword]])\n",
    "\n",
    "    if train_on_gpu:\n",
    "        subword, metadata = subword.to(device='cuda', dtype=torch.long), \\\n",
    "                            metadata.to(device='cuda', dtype=torch.long)\n",
    "\n",
    "    # detach hidden state from history:\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    # get the output of the model:\n",
    "    out, h = net(subword, metadata, torch.tensor([1]), h) # Batch_size = 1\n",
    "    \n",
    "    # get the subwords probabilities\n",
    "    # apply softmax to get p probabilities for the likely next subword giving x:  \n",
    "    out = out[:, allowed_idx] # size: [1, len(vocab)-len(allowed_idx)]\n",
    "    p = F.softmax(out/temp, dim=1).data \n",
    "    if train_on_gpu:\n",
    "        p = p.cpu() # move to cpu\n",
    "\n",
    "    # get top subwords\n",
    "    # considering the k most probable subwords with topk method:\n",
    "    if top_k is None:\n",
    "        top_sw = np.arange(p.size()[-1])\n",
    "    else:\n",
    "        p, top_sw = p.topk(top_k)\n",
    "        top_sw = top_sw.numpy().squeeze()\n",
    "\n",
    "    # select the likely next subword with some element of randomness:\n",
    "    p = p.numpy().squeeze()\n",
    "    subword = np.random.choice(top_sw, p=p/p.sum())\n",
    "\n",
    "    # return the encoded value of the predicted subword and the hidden state:\n",
    "    return subword, h\n",
    "  \n",
    "\n",
    "def sample(net, size=300, num_sents=5, prime='this movie', temp=0.9,\n",
    "           metadata=None, top_k=None, indices_to_ignore=None):\n",
    "    \n",
    "    if metadata is None:\n",
    "        metadata = [1, 5, 8, 11, 13, 17]\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    vocab_size = len(net.subwords)\n",
    "    filtered_vocab_map, allowed_idx = {}, []\n",
    "    if indices_to_ignore is not None:\n",
    "        idx_filtered = 0        \n",
    "        for idx in range(vocab_size):\n",
    "            if idx not in indices_to_ignore:\n",
    "                allowed_idx.append(idx)\n",
    "                filtered_vocab_map[idx_filtered] = idx\n",
    "                idx_filtered += 1\n",
    "        allowed_idx = torch.tensor(allowed_idx)\n",
    "    else:\n",
    "        allowed_idx = torch.tensor(list(range(vocab_size)))\n",
    "        filtered_vocab_map = {i:i for i in range(vocab_size)}\n",
    "    filtered_vocab_map_inverse = {v: k for k, v in filtered_vocab_map.items()}\n",
    "\n",
    "    # First off, run through the prime subwords:\n",
    "    subwords = net.bpe.encode(prime, output_type=yttm.OutputType.ID)\n",
    "    metadata = torch.tensor([[metadata]]) # Reshaping to (1, 1, 6) so that the dims correspond to those from get_batches()\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    for sw in subwords:\n",
    "        subword, h = predict(net, sw, metadata, h, temp=temp, top_k=top_k, allowed_idx=allowed_idx)\n",
    "        subword = filtered_vocab_map[subword]\n",
    "    subwords.append(subword) # appending the subword that comes after the last prime subword\n",
    "    \n",
    "    # Now pass in the previous subword and get a new one\n",
    "    i = 0\n",
    "    eos_idx = filtered_vocab_map_inverse[3] # 3 is the index of '<EOS>'\n",
    "    while (subwords.count(eos_idx) < num_sents) and (i < size): # We predict 'num_sents' until 'size' is not exceeded  \n",
    "        subword, h = predict(net, subwords[-1], metadata, h, temp=temp, top_k=top_k, allowed_idx=allowed_idx)\n",
    "        subword = filtered_vocab_map[subword]\n",
    "        subwords.append(subword)\n",
    "        i += 1 \n",
    "        \n",
    "    #subwords = list(filter(lambda a: a != 3, subwords)) # remove all occurrences of '<EOS>' from the final output\n",
    "    return net.bpe.decode(subwords)[0]\n",
    "\n",
    "\n",
    "### Cell 9: ###\n",
    "class SubwordRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, subwords=None, bpe=None, embedding_dims=None, \\\n",
    "                 n_hidden=1024, n_layers=2, drop_prob=0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # saving the bpe-model to be able to use its encoding/decoding functions when generating some text:\n",
    "        self.bpe = bpe\n",
    "        self.subwords = subwords\n",
    "        \n",
    "        # defining 2 Embedding layers: one for subwords and another for metadata:\n",
    "        if embedding_dims is None:\n",
    "            embedding_dims = [256, 20]\n",
    "            \n",
    "        subword_emb_dim, metadata_emb_dim = embedding_dims\n",
    "        \n",
    "        self.sw_emb_layer = nn.Embedding(len(self.subwords), subword_emb_dim)\n",
    "        self.md_emb_layer = nn.Embedding(19, metadata_emb_dim) # 18 - total number of metadata features + 1 for padded sequences\n",
    "        \n",
    "        embedding_dim = subword_emb_dim + 6*metadata_emb_dim\n",
    "        # defining the LSTM:\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # define a dropout layer:\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # define the final, fully-connected output layer:\n",
    "        self.fc = nn.Linear(n_hidden, len(self.subwords))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, x_meta, batch_lengths, hidden):\n",
    "        \"\"\" \n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        # Creating an embedding by concatenating a subword embedding and a metadata embedding:\n",
    "        subword_embeds = self.sw_emb_layer(x)\n",
    "        meta_embeds = self.md_emb_layer(x_meta).reshape(*x.size(), -1)\n",
    "    \n",
    "        embeds = torch.cat((subword_embeds, meta_embeds), dim=2)\n",
    "        \n",
    "        # Clamping because pack_padded_sequence doesn't support empty batches\n",
    "        batch_lengths_clamped = batch_lengths.clamp(min=1, max=batch_lengths.max().item())\n",
    "        \n",
    "        # Packing the sequence to avoid many senseless computations:\n",
    "        packed = pack_padded_sequence(embeds, batch_lengths_clamped, batch_first=True)\n",
    "        \n",
    "        # Get the outputs and the new hidden state from the lstm:\n",
    "        r_output_packed, hidden = self.lstm(packed, hidden)\n",
    "        \n",
    "        # Masking hidden, where not clamped batch_size = 0: \n",
    "        mask = (batch_lengths == 0).view(-1, 1)\n",
    "        if train_on_gpu:\n",
    "            mask = mask.to(device='cuda') #.to(device='cuda') #.cuda()\n",
    "        hidden[0].masked_fill_(mask, 0) # Masking hidden but not the cell state! # Sure that this has to be done?\n",
    "        \n",
    "        # Unpack the output:\n",
    "        r_output, _ = pad_packed_sequence(r_output_packed, batch_first=True)\n",
    "        \n",
    "        # pass through a dropout layer:\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view:\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        # put x through the fully-connected layer:\n",
    "        out = self.fc(out)       \n",
    "        \n",
    "        # return the final output and the hidden state:\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state \n",
    "        \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM:\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device='cuda'),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device='cuda'))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "### Cell 10: ###\n",
    "def train(net, train_data, train_metadata, epochs=10, batch_size=16, \n",
    "          seq_length=160, lr=0.0001, clip=5, val_frac=0.1, print_every=1, \n",
    "          current_epoch=1, save_every=2, shuffle=False):\n",
    "    \"\"\" \n",
    "    Training a network \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        \n",
    "    net: SubwordRNN network\n",
    "    data: text data to train the network\n",
    "    epochs: Number of epochs to train\n",
    "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "    seq_length: Number of character steps per mini-batch\n",
    "    lr: learning rate\n",
    "    clip: gradient clipping\n",
    "    val_frac: Fraction of data to hold out for validation\n",
    "    print_every: Number of steps for printing training and validation loss   \n",
    "    shuffle: Whether to shuffle the training data after each iteration\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Create training and validation data:\n",
    "    corpus_len = len(train_data)\n",
    "    array = np.arange(start=0, stop=corpus_len) # review indices range   \n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    train_size = int((1-val_frac)*corpus_len)\n",
    "    val_size = corpus_len - train_size\n",
    "    train_set_idx = rng.choice(array, size=train_size, replace=False)\n",
    "\n",
    "    val_set_idx = np.setdiff1d(array, train_set_idx) # Unique values in array1 that are not in train_set_idx\n",
    "    \n",
    "    data = deepcopy([train_data[idx] for idx in train_set_idx])\n",
    "    metadata = deepcopy([train_metadata[idx] for idx in train_set_idx])\n",
    "    val_data = deepcopy([train_data[idx] for idx in val_set_idx])    \n",
    "    val_metadata = deepcopy([train_metadata[idx] for idx in val_set_idx])\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.to(device='cuda')\n",
    "    \n",
    "    counter = 0\n",
    "    n_subwords = len(net.subwords)\n",
    "    for e in range(current_epoch, epochs+current_epoch):\n",
    "        # Initialize hidden state:\n",
    "        h = net.init_hidden(batch_size)\n",
    "        losses = []\n",
    "        \n",
    "        for inputs, meta, targets, batch_sizes in get_batches(data, metadata, batch_size, seq_length, shuffle):\n",
    "            counter += 1\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history:\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs = inputs.to(device='cuda', dtype=torch.long)\n",
    "                meta = meta.to(device='cuda', dtype=torch.long)\n",
    "                targets = targets.to(device='cuda', dtype=torch.long)\n",
    "                \n",
    "            seq_length_real_train = targets.size()[-1]\n",
    "\n",
    "            # Zero accumulated gradients:\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # Get the output from the model:\n",
    "            output, h = net(inputs, meta, batch_sizes, h)\n",
    "\n",
    "            # Calculate the loss and perform backprop:\n",
    "            loss = criterion(output, targets.reshape(batch_size*seq_length_real_train))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs:\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "        # Loss stats:\n",
    "        if e % print_every == 0:\n",
    "            # Get validation loss:\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for x, x_meta, y, bs in get_batches(val_data, val_metadata, batch_size, seq_length, shuffle):                \n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                if train_on_gpu:\n",
    "                    inputs = x.to(device='cuda', dtype=torch.long)\n",
    "                    meta = x_meta.to(device='cuda', dtype=torch.long)\n",
    "                    targets = y.to(device='cuda', dtype=torch.long)\n",
    "                    \n",
    "                seq_length_real_val = targets.size()[-1]\n",
    "                output, val_h = net(inputs, meta, bs, val_h)\n",
    "                val_loss = criterion(output, targets.reshape(batch_size*seq_length_real_val))\n",
    "            \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            net.train() # reset to train mode after iterating through validation data\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Printing out some training statistics:\n",
    "            print(f'Epoch: {e:2}/{epochs}...',\n",
    "                  f'Step: {counter:6}...',\n",
    "                  f'Mean_Loss: {np.mean(losses):.3f}...',\n",
    "                  f'Val_Loss: {mean_val_loss:.3f}')\n",
    "            \n",
    "            # Generating text:\n",
    "            generated = sample(net, size=300, prime='this movie', metadata=[2, 5, 7, 10, 12, 18], \n",
    "                               top_k=None, indices_to_ignore=[0, 1]) # generating text without '<PAD>', '<UNK>'          \n",
    "            generated = f'Generated at epoch {e}:\\n{generated}\\n\\n' # generated text with a header\n",
    "            \n",
    "            save_dir = 'Generated Text' # directory where the generated text at epoch e will be saved\n",
    "            if not os.path.isdir(save_dir): # if there is no such directory, one will be created\n",
    "                os.mkdir(save_dir) \n",
    "            with open(f'{save_dir}/generated_text.txt', 'a', encoding='utf-8', errors='replace') as f:\n",
    "                f.write(generated)\n",
    "                \n",
    "        # Saving our model:        \n",
    "        if e % save_every == 0 or e == epochs:\n",
    "            net.eval() # VERY IMPORTANT\n",
    "            \n",
    "            if e >= 1 and e < 10:\n",
    "                model = f'lstm_0{e}_epoch.pt'\n",
    "            else:\n",
    "                model = f'lstm_{e}_epoch.pt'\n",
    "            \n",
    "            checkpoint = {'epoch': e,\n",
    "                          'n_hidden': net.n_hidden,\n",
    "                          'n_layers': net.n_layers,\n",
    "                          'model_state_dict': net.state_dict(),\n",
    "                          'opt_state_dict': opt.state_dict(),\n",
    "                          'loss': loss}\n",
    "\n",
    "            with open(model, 'wb') as f:\n",
    "                torch.save(checkpoint, f)\n",
    "                \n",
    "            net.train() # VERY IMPORTANT   \n",
    "\n",
    "\n",
    "### Cell 11: ###\n",
    "# Defining and printing out the NN properties:\n",
    "embedding_dims = [256, 20] #[128, 10]\n",
    "drop_prob = 0.5 #0.5\n",
    "n_hidden= 1024 #1024\n",
    "n_layers=3 #2\n",
    "lr=0.0001 #0.001 seems to overfit\n",
    "\n",
    "net = SubwordRNN(subwords=subwords, bpe=bpe, embedding_dims=embedding_dims, \n",
    "                 n_hidden=n_hidden, n_layers=n_layers, drop_prob=drop_prob)\n",
    "print(net)\n",
    "\n",
    "batch_size = 64 #16 \n",
    "seq_length = 160\n",
    "n_epochs = 10 #2\n",
    "\n",
    "# Training the model:\n",
    "#train(net, encoded_train, metadata_train, epochs=n_epochs, \n",
    "      #batch_size=batch_size, seq_length=seq_length, lr=lr, \n",
    "      #print_every=1, save_every=5, current_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubwordRNN(\n",
      "  (sw_emb_layer): Embedding(30469, 256)\n",
      "  (md_emb_layer): Embedding(19, 20)\n",
      "  (lstm): LSTM(376, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=30469, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "path_to_model = 'lstm_10_epoch_lr0001.pt'\n",
    "\n",
    "checkpoint = torch.load(path_to_model)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(net)\n",
    "#train(net, encoded_train, metadata_train, epochs=n_epochs, batch_size=batch_size, \n",
    "      #seq_length=seq_length, lr=lr, print_every=1, save_every=1, current_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DwmmYxPJuxQQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+\n",
      "|       Modules       | Parameters |\n",
      "+---------------------+------------+\n",
      "| sw_emb_layer.weight |  7800064   |\n",
      "| md_emb_layer.weight |    380     |\n",
      "|  lstm.weight_ih_l0  |  1540096   |\n",
      "|  lstm.weight_hh_l0  |  4194304   |\n",
      "|   lstm.bias_ih_l0   |    4096    |\n",
      "|   lstm.bias_hh_l0   |    4096    |\n",
      "|  lstm.weight_ih_l1  |  4194304   |\n",
      "|  lstm.weight_hh_l1  |  4194304   |\n",
      "|   lstm.bias_ih_l1   |    4096    |\n",
      "|   lstm.bias_hh_l1   |    4096    |\n",
      "|  lstm.weight_ih_l2  |  4194304   |\n",
      "|  lstm.weight_hh_l2  |  4194304   |\n",
      "|   lstm.bias_ih_l2   |    4096    |\n",
      "|   lstm.bias_hh_l2   |    4096    |\n",
      "|      fc.weight      |  31200256  |\n",
      "|       fc.bias       |   30469    |\n",
      "+---------------------+------------+\n",
      "Total Trainable Params: 61567361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61567361"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_experiments = [[2, 5, 8, 11, 12, 17], \n",
    "                        [1, 5, 8, 11, 12, 17], \n",
    "                        [2, 3, 8, 11, 12, 17], \n",
    "                        [2, 5, 6, 11, 12, 17], \n",
    "                        [2, 5, 8, 10, 12, 17], \n",
    "                        [2, 5, 8, 11, 15, 17], \n",
    "                        [2, 5, 8, 11, 12, 18]]\n",
    "\n",
    "for cur_meta in metadata_experiments:\n",
    "    generated = sample(net, size=300, num_sents=2, prime='this movie', temp=0.6,\n",
    "                       metadata=cur_meta, top_k=None)\n",
    "    generated = f'Metadata: {\" \".join([str(el) for el in cur_meta])}\\n{generated}\\n\\n'\n",
    "    with open(f'experiments.txt', 'a', encoding='utf-8', errors='replace') as f:\n",
    "        f.write(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the trained model: 501.74\n"
     ]
    }
   ],
   "source": [
    "def perplexity(net, encoded_test, metadata_test, batch_size=16):\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "        \n",
    "    net.eval()    \n",
    "    h = net.init_hidden(batch_size) \n",
    "    logs_sum, N = 0, 0\n",
    "    \n",
    "    for x, x_meta, y, bs in get_batches(encoded_test, metadata_test):                \n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs = x.to(device='cuda', dtype=torch.long)\n",
    "            meta = x_meta.to(device='cuda', dtype=torch.long)\n",
    "            targets = y.to(device='cuda', dtype=torch.long)\n",
    "\n",
    "        seq_length_real_val = targets.size()[-1]\n",
    "        N += bs.sum().item()\n",
    "        output, h = net(inputs, meta, bs, h)\n",
    "        loss = F.cross_entropy(output, targets.reshape(batch_size*seq_length_real_val), \n",
    "                               ignore_index=0, reduction='sum')\n",
    "        logs_sum += loss.item()\n",
    "        \n",
    "    prplxt = np.exp(logs_sum/N)\n",
    "    \n",
    "    return prplxt\n",
    "\n",
    "print(f'Perplexity of the trained model: {perplexity(net, encoded_test, metadata_test):.2f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GeneratorNN_ver3.ipynb",
   "provenance": [
    {
     "file_id": "1VIR6Br6-iUSeKYCVMfS_hcMyVP2MwybR",
     "timestamp": 1600709535732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
